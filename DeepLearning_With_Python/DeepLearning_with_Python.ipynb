{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hasanmog/Journey-Through-AI-Learning/blob/main/DeepLearning_With_Python/DeepLearning_with_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning for ComputerVision\n"
      ],
      "metadata": {
        "id": "A7x50IUvyjhS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "working with the MNIST dataset (handwritten numbers)"
      ],
      "metadata": {
        "id": "AB-wvriDCpsW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flxi06ask0fh"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers , models , datasets , utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "\n",
        "# NN\n",
        "model.add(layers.Conv2D(32 , (3 , 3) , activation = 'relu' , input_shape = (28,28 , 1))) # (3,3) is the size of the window where the conv learn local patterns\n",
        "model.add(layers.MaxPooling2D(2 , 2))\n",
        "model.add(layers.Conv2D(64 , (3,3) , activation = 'relu'))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Conv2D(64 , (3,3) , activation = 'relu'))\n"
      ],
      "metadata": {
        "id": "RVxatjJ2xshV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "izWpOMHxzrhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In order to make the prediction , we need a densley connected classifier\n",
        "# Dense layer takes 1D tensor only !\n",
        "# Therfore we need to transform the data coming from the NN(3D) to 1D\n",
        "\n",
        "model.add(layers.Flatten()) # used to convert to 1D --> (x , y , z) --> (x * y * z)\n",
        "model.add(layers.Dense(64 , activation = 'relu'))\n",
        "model.add(layers.Dense(10 , activation = 'softmax'))"
      ],
      "metadata": {
        "id": "JM9h3HPFz8bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "rwx2rZEK0uoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train , y_train) , (x_test , y_test) = keras.datasets.mnist.load_data() # X --> images(inputs) , y --> labels (target)"
      ],
      "metadata": {
        "id": "reugHmNe1CV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)\n",
        "# we have 60,000 training samples"
      ],
      "metadata": {
        "id": "MPaWo35u2QxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train.shape)\n",
        "print(y_train[:10])"
      ],
      "metadata": {
        "id": "ZuzpVSy-3E2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.reshape((60000 , 28 , 28 , 1)) # adding the channel dimension which is 1 here (grey-scale)\n",
        "x_test = x_test.reshape((10000 , 28 , 28 , 1))\n",
        "\n",
        "x_train = x_train / 255 # normalize\n",
        "x_test = x_test / 255\n",
        "\n",
        "y_train = utils.to_categorical(y_train) # Converts a class vector (integers) to binary class matrix.\n",
        "print(y_train[:10])\n",
        "print(y_train.shape)\n",
        "\n",
        "y_test = utils.to_categorical(y_test)\n"
      ],
      "metadata": {
        "id": "abDqh5Ju2hyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = 'rmsprop' , loss = 'categorical_crossentropy' , metrics = ['accuracy'] )\n",
        "model.fit(x_train , y_train , batch_size = 64 , epochs = 5 )"
      ],
      "metadata": {
        "id": "A0qULePY4WF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a ConvNet from Scratch"
      ],
      "metadata": {
        "id": "ewWqZzV4CxJH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing dataset from Kaggle"
      ],
      "metadata": {
        "id": "7mhBP72rfFTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "id": "aT6F2fxrdI4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['KAGGLE_USERNAME'] =   # replace with your Kaggle username\n",
        "os.environ['KAGGLE_KEY'] =   # replace with your Kaggle key\n"
      ],
      "metadata": {
        "id": "FYQm0sSVdLFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d biaiscience/dogs-vs-cats # after -d replace with owner_name/dataset_name"
      ],
      "metadata": {
        "id": "353-Pn_8dWgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip dogs-vs-cats.zip"
      ],
      "metadata": {
        "id": "P3HHyeMud7Kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Prep"
      ],
      "metadata": {
        "id": "atUFl3_NfLGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os , shutil"
      ],
      "metadata": {
        "id": "b7_pNbPCd85E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir(\"dataset\")\n",
        "os.mkdir(\"dataset/train\")\n",
        "os.mkdir(\"dataset/val\")\n",
        "os.mkdir(\"dataset/test\")"
      ],
      "metadata": {
        "id": "2tGXBEVevmS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orig_train_dir = 'train/train'\n",
        "orig_test_dir = 'test/test'"
      ],
      "metadata": {
        "id": "fZB978Mjw5pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "animals = ['cat' , 'dog']\n",
        "samples = 1000 # train 1000 , val 500 , test 500\n",
        "sample_per_animal = int(samples/len(animals))\n",
        "\n",
        "train_dir = 'dataset/train/'\n",
        "val_dir = 'dataset/val'\n",
        "test_dir = 'dataset/test'\n",
        "\n",
        "for animal in animals:\n",
        "  print(f\"now {animal}\")\n",
        "  for sample in range(sample_per_animal ):\n",
        "    ex = f\"{animal}.{sample }.jpg\" # when adding samples to the val_dir , add 1000 to the sample\n",
        "    print(ex)\n",
        "    src = os.path.join(orig_train_dir , ex)\n",
        "    dest = os.path.join(train_dir , ex) # replace train_dir with val_dir for validation split\n",
        "    shutil.copyfile(src , dest)"
      ],
      "metadata": {
        "id": "eq8X70lR1fDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = 499\n",
        "test_samples = os.listdir(orig_test_dir)\n",
        "s = test_samples[:samples+1]\n",
        "\n",
        "for sample in s:\n",
        "\n",
        "  src = os.path.join(orig_test_dir , sample)\n",
        "  dest = os.path.join(test_dir , sample )\n",
        "  shutil.copyfile(src , dest)\n"
      ],
      "metadata": {
        "id": "79mRedGN3TkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_samples = os.listdir(train_dir)\n",
        "# note , for test it will return 0 , since the samples are not named after the animal\n",
        "val_samples = os.listdir(val_dir)\n",
        "test_samples = os.listdir(test_dir)\n",
        "\n",
        "print(\"number of training samples\" , len(train_samples))\n",
        "print(\"number of val samples\" , len(val_samples))\n",
        "print(\"number of test samples\" , len(test_samples))\n",
        "\n",
        "dogs = 0\n",
        "cats = 0\n",
        "\n",
        "for train in train_samples : #change the train_samples to val_samples\n",
        "\n",
        "  if train.startswith('dog'):\n",
        "    dogs+=1\n",
        "\n",
        "  if train.startswith('cat'):\n",
        "    cats+=1\n",
        "\n",
        "\n",
        "print(\"number of dogs samples\" , dogs)\n",
        "print(\"number of cats samples\" , cats)"
      ],
      "metadata": {
        "id": "bSYw_CnL8YRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "img = cv2.imread(os.path.join(train_dir , train_samples[10]))\n",
        "\n",
        "size = (250 , 250)\n",
        "\n",
        "img = cv2.resize(img , size)"
      ],
      "metadata": {
        "id": "JXxS01cNDF_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "cv2_imshow(img)"
      ],
      "metadata": {
        "id": "uJvQqT3jECjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_samples[0])"
      ],
      "metadata": {
        "id": "zzGM0ligH4rU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "base_train_dir = \"dataset/train\"\n",
        "base_val_dir = \"dataset/val\"\n",
        "base_test_dir = \"dataset/test\"\n",
        "\n",
        "dogs = []\n",
        "cats = []\n",
        "\n",
        "for m in os.listdir(base_train_dir):\n",
        "\n",
        "  if  m.startswith('dog'):\n",
        "    dogs.append(m)\n",
        "\n",
        "  if m.startswith('cat'):\n",
        "    cats.append(m)\n",
        "\n",
        "# train_dogs_dir = os.mkdir(\"dataset/train/dogs\")\n",
        "# train_cats_dir = os.mkdir(\"dataset/train/cats\")\n",
        "# val_dogs_dir = os.mkdir(\"dataset/val/dogs\")\n",
        "# val_cats_dir = os.mkdir(\"dataset/val/cats\")\n",
        "\n",
        "for i in dogs:\n",
        "  src = os.path.join(base_train_dir , i)\n",
        "  dest = \"dataset/train/dogs\"\n",
        "  shutil.move(src , dest)\n",
        "\n",
        "for i in cats:\n",
        "  src = os.path.join(base_train_dir , i)\n",
        "  dest = \"dataset/train/cats\"\n",
        "  shutil.move(src , dest)"
      ],
      "metadata": {
        "id": "8cmehFVPzjg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building Network"
      ],
      "metadata": {
        "id": "X4J0rSksCLa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(32  , (3,3) , activation = 'relu' , input_shape = (250 , 250 ,3)))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Conv2D(64 , (3,3) , activation = 'relu'))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Conv2D(128, (3,3) , activation = 'relu'))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Conv2D(128 , (3,3) , activation = 'relu'))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Conv2D(64 , (3,3) , activation = 'relu'))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(512 , activation = 'relu'))\n",
        "model.add(layers.Dense(1 , activation = 'sigmoid'))"
      ],
      "metadata": {
        "id": "LRtd9Sb_DB0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "rsLTXZjTu0Sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import optimizers\n",
        "\n",
        "model.compile(optimizer = optimizers.RMSprop(learning_rate = 1e-4) , loss = 'binary_crossentropy' , metrics = ['acc'] )"
      ],
      "metadata": {
        "id": "WqdCPtf0wKXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = 'dataset/train/'\n",
        "val_dir = 'dataset/val'\n",
        "test_dir = 'dataset/test'"
      ],
      "metadata": {
        "id": "LA29iYAzymz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DataLoading\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(train_dir ,\n",
        "                                                    target_size = (250,250) ,\n",
        "                                                    batch_size = 20 ,\n",
        "                                                    class_mode = 'binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(val_dir ,\n",
        "                                                        target_size = (250,250) ,\n",
        "                                                        batch_size = 25 ,\n",
        "                                                        class_mode = 'binary')"
      ],
      "metadata": {
        "id": "oXh9htKqwpW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing dataloader:\n",
        "\n",
        "for data_batch , labels_batch in train_generator :\n",
        "  print('data' , data_batch.shape)\n",
        "  print('labels' , labels_batch.shape)\n",
        "  break"
      ],
      "metadata": {
        "id": "UMTFQGktykNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "history = model.fit_generator(train_generator ,\n",
        "                              steps_per_epoch = 50 ,\n",
        "                              epochs = 10 ,\n",
        "                              validation_data = validation_generator ,\n",
        "                              validation_steps = 20)\n",
        "\n",
        "model.save('cats_and_dogs_small_1.h5')"
      ],
      "metadata": {
        "id": "URm7_Jg82s9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('model_history.json' , 'w') as f :\n",
        "  json.dump(history.history , f)"
      ],
      "metadata": {
        "id": "yIEyge01GNZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1 , len(acc)+1)\n",
        "\n",
        "plt.plot(epochs , acc , 'bo' , label = 'Training Acc')\n",
        "plt.plot(epochs , val_acc , 'b' , label = 'Val Acc')\n",
        "plt.title(\"Training vs Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs , loss , 'bo' , label = 'Training loss')\n",
        "plt.plot(epochs , val_loss , 'b' , label = 'Val loss')\n",
        "plt.title(\"Training vs Validation loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OPLBN9ls3crD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see clearly , We have overfitting problem"
      ],
      "metadata": {
        "id": "Rtslxq7mGhoG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding DropOut layer and Data Augmentation to increase dataset"
      ],
      "metadata": {
        "id": "KTSIRy9iG_pb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(32  , (3,3) , activation = 'relu' , input_shape = (250 , 250 ,3)))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Conv2D(64 , (3,3) , activation = 'relu'))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Conv2D(128, (3,3) , activation = 'relu'))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Conv2D(128 , (3,3) , activation = 'relu'))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Conv2D(64 , (3,3) , activation = 'relu'))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(512 , activation = 'relu'))\n",
        "model.add(layers.Dense(1 , activation = 'sigmoid'))\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "YRfW_anQIFgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Augmentation\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale = 1./255 ,\n",
        "    rotation_range = 40 ,\n",
        "    width_shift_range = 0.2 ,\n",
        "    height_shift_range = 0.2 ,\n",
        "    shear_range = 0.2 ,\n",
        "    zoom_range = 0.2 ,\n",
        "    horizontal_flip = True\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(train_dir ,\n",
        "                                                    target_size = (250,250) ,\n",
        "                                                    batch_size = 32,\n",
        "                                                    class_mode = 'binary')"
      ],
      "metadata": {
        "id": "OY53m5hgGfoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit_generator(\n",
        "    train_generator ,\n",
        "    steps_per_epoch = 100,\n",
        "    epochs = 100 ,\n",
        "    validation_data = validation_generator ,\n",
        "    validation_steps = 50\n",
        ")\n",
        "\n",
        "model.save('cats_and_dogs_small_2.h5')"
      ],
      "metadata": {
        "id": "2b6az-kvIXgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('model_history_2.json' , 'w') as f :\n",
        "\n",
        "  json.dump(history.history , f)"
      ],
      "metadata": {
        "id": "cfUzXzzsIvmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1 , len(acc)+1)\n",
        "\n",
        "plt.plot(epochs , acc , 'bo' , label = 'Training Acc')\n",
        "plt.plot(epochs , val_acc , 'b' , label = 'Val Acc')\n",
        "plt.title(\"Training vs Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs , loss , 'bo' , label = 'Training loss')\n",
        "plt.plot(epochs , val_loss , 'b' , label = 'Val loss')\n",
        "plt.title(\"Training vs Validation loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Mmwf2VyUJC9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see the performance increased significantly compared to unaugmented model.\n",
        "\n",
        "By applying L1 or L2 regularization will enhance the results more."
      ],
      "metadata": {
        "id": "phdP1GICYeBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using a Pretrained Model"
      ],
      "metadata": {
        "id": "o9IorytYYuGe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For Feature Extraction\n",
        "\n",
        "will use pretrained [VGG16](https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg16/VGG16).\n",
        "[Architecture](https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.geeksforgeeks.org%2Fvgg-16-cnn-model%2F&psig=AOvVaw18JAs0D2qlc6YD2pWEy9WT&ust=1703869812399000&source=images&cd=vfe&opi=89978449&ved=0CBIQjRxqFwoTCKCkvevPsoMDFQAAAAAdAAAAABAI)\n"
      ],
      "metadata": {
        "id": "Fi0L6h4AY8kr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications import VGG16\n",
        "\n",
        "conv_base = VGG16(\n",
        "    include_top = False , # classifer\n",
        "    weights = 'imagenet' ,\n",
        "    input_shape = (250 , 250 , 3)\n",
        ")"
      ],
      "metadata": {
        "id": "pXK-Ik_eYm_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary()"
      ],
      "metadata": {
        "id": "HeMEoJMPbLJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256 , activation = 'relu'))\n",
        "model.add(layers.Dense(1 , activation = 'sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "P_zAE24IbUl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import optimizers\n",
        "\n",
        "model.compile(optimizer = optimizers.RMSprop(learning_rate = 2e-5) , loss = 'binary_crossentropy' , metrics = ['acc'])"
      ],
      "metadata": {
        "id": "S-2x-ZD6ds2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Needs GPU to run it\n",
        "\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator ,\n",
        "    steps_per_epoch = 100 ,\n",
        "    epochs = 30,\n",
        "    validation_data = validation_generator ,\n",
        "    validation_steps = 50\n",
        ")"
      ],
      "metadata": {
        "id": "hlTyRSIJdbOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1 , len(acc)+1)\n",
        "\n",
        "plt.plot(epochs , acc , 'bo' , label = 'Training Acc')\n",
        "plt.plot(epochs , val_acc , 'b' , label = 'Val Acc')\n",
        "plt.title(\"Training vs Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs , loss , 'bo' , label = 'Training loss')\n",
        "plt.plot(epochs , val_loss , 'b' , label = 'Val loss')\n",
        "plt.title(\"Training vs Validation loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "d90RmHrpeWD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FineTuning"
      ],
      "metadata": {
        "id": "sEps6a-BegUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.trainable = True\n",
        "\n",
        "set_trainable = False\n",
        "\n",
        "for layer in conv_base.layers:\n",
        "  if layer == 'block5_conv1':\n",
        "    set_trainable = True\n",
        "\n",
        "  if set_trainable:\n",
        "    layer.trainable = True\n",
        "\n",
        "  else :\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "JFDxNfXgeiF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss = 'binary_crossentropy' , optimizer = optimizers.RMSprop(learning_rate = 1e-5) , metrics = ['acc'])"
      ],
      "metadata": {
        "id": "HWgSWhLIfgrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#needs GPU\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator ,\n",
        "    steps_per_epoch = 100 ,\n",
        "    epochs = 100 ,\n",
        "    validation_data = validation_generator ,\n",
        "    validation_steps = 50\n",
        ")"
      ],
      "metadata": {
        "id": "xgJg6_47fxw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir ,\n",
        "    target_size = (250 ,250) ,\n",
        "    batch_size = 20,\n",
        "    class_mode = 'binary'\n",
        ")\n",
        "\n",
        "test_loss , test_acc = model.evaluate_generator(test_generator , steps = 50)\n",
        "print(\"test_acc :\" , test_acc)"
      ],
      "metadata": {
        "id": "y7z6WvYJgDkG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}