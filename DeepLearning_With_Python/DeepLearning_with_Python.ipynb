{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "A7x50IUvyjhS",
        "yYThv5Vj9rcM",
        "7soJ0fNs32Ts",
        "xkLzL6vPi458",
        "-oPKiR_wjOxv"
      ],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hasanmog/Journey-Through-AI-Learning/blob/main/DeepLearning_With_Python/DeepLearning_with_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning for ComputerVision\n"
      ],
      "metadata": {
        "id": "A7x50IUvyjhS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "working with the MNIST dataset (handwritten numbers)"
      ],
      "metadata": {
        "id": "AB-wvriDCpsW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flxi06ask0fh"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers , models , datasets , utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "\n",
        "# NN\n",
        "model.add(layers.Conv2D(32 , (3 , 3) , activation = 'relu' , input_shape = (28,28 , 1))) # (3,3) is the size of the window where the conv learn local patterns\n",
        "model.add(layers.MaxPooling2D(2 , 2))\n",
        "model.add(layers.Conv2D(64 , (3,3) , activation = 'relu'))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Conv2D(64 , (3,3) , activation = 'relu'))\n"
      ],
      "metadata": {
        "id": "RVxatjJ2xshV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "izWpOMHxzrhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In order to make the prediction , we need a densley connected classifier\n",
        "# Dense layer takes 1D tensor only !\n",
        "# Therfore we need to transform the data coming from the NN(3D) to 1D\n",
        "\n",
        "model.add(layers.Flatten()) # used to convert to 1D --> (x , y , z) --> (x * y * z)\n",
        "model.add(layers.Dense(64 , activation = 'relu'))\n",
        "model.add(layers.Dense(10 , activation = 'softmax'))"
      ],
      "metadata": {
        "id": "JM9h3HPFz8bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "rwx2rZEK0uoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train , y_train) , (x_test , y_test) = keras.datasets.mnist.load_data() # X --> images(inputs) , y --> labels (target)"
      ],
      "metadata": {
        "id": "reugHmNe1CV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)\n",
        "# we have 60,000 training samples"
      ],
      "metadata": {
        "id": "MPaWo35u2QxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train.shape)\n",
        "print(y_train[:10])"
      ],
      "metadata": {
        "id": "ZuzpVSy-3E2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.reshape((60000 , 28 , 28 , 1)) # adding the channel dimension which is 1 here (grey-scale)\n",
        "x_test = x_test.reshape((10000 , 28 , 28 , 1))\n",
        "\n",
        "x_train = x_train / 255 # normalize\n",
        "x_test = x_test / 255\n",
        "\n",
        "y_train = utils.to_categorical(y_train) # Converts a class vector (integers) to binary class matrix.\n",
        "print(y_train[:10])\n",
        "print(y_train.shape)\n",
        "\n",
        "y_test = utils.to_categorical(y_test)\n"
      ],
      "metadata": {
        "id": "abDqh5Ju2hyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = 'rmsprop' , loss = 'categorical_crossentropy' , metrics = ['accuracy'] )\n",
        "model.fit(x_train , y_train , batch_size = 64 , epochs = 5 )"
      ],
      "metadata": {
        "id": "A0qULePY4WF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a ConvNet from Scratch"
      ],
      "metadata": {
        "id": "ewWqZzV4CxJH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing dataset from Kaggle"
      ],
      "metadata": {
        "id": "7mhBP72rfFTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "id": "aT6F2fxrdI4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['KAGGLE_USERNAME'] =  \"hasanmog\"  # replace with your Kaggle username\n",
        "os.environ['KAGGLE_KEY'] =  \"f3ccafa264265fe4f3baf59bda89bd0f\" # replace with your Kaggle key\n"
      ],
      "metadata": {
        "id": "FYQm0sSVdLFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d biaiscience/dogs-vs-cats # after -d replace with owner_name/dataset_name"
      ],
      "metadata": {
        "id": "353-Pn_8dWgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip dogs-vs-cats.zip"
      ],
      "metadata": {
        "id": "P3HHyeMud7Kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Prep"
      ],
      "metadata": {
        "id": "atUFl3_NfLGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os , shutil"
      ],
      "metadata": {
        "id": "b7_pNbPCd85E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir(\"dataset\")\n",
        "os.mkdir(\"dataset/train\")\n",
        "os.mkdir(\"dataset/val\")\n",
        "os.mkdir(\"dataset/test\")"
      ],
      "metadata": {
        "id": "2tGXBEVevmS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orig_train_dir = 'train/train'\n",
        "orig_test_dir = 'test/test'"
      ],
      "metadata": {
        "id": "fZB978Mjw5pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "animals = ['cat' , 'dog']\n",
        "samples = 500 # train 1000 , val 500 , test 500\n",
        "sample_per_animal = int(samples/len(animals))\n",
        "\n",
        "train_dir = 'dataset/train/'\n",
        "val_dir = 'dataset/val'\n",
        "test_dir = 'dataset/test'\n",
        "\n",
        "for animal in animals:\n",
        "  print(f\"now {animal}\")\n",
        "  for sample in range(sample_per_animal ):\n",
        "    ex = f\"{animal}.{sample+1000  }.jpg\" # when adding samples to the val_dir , add 1000 to the sample\n",
        "    print(ex)\n",
        "    src = os.path.join(orig_train_dir , ex)\n",
        "    dest = os.path.join(val_dir , ex) # replace train_dir with val_dir for validation split\n",
        "    shutil.copyfile(src , dest)"
      ],
      "metadata": {
        "id": "eq8X70lR1fDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = 499\n",
        "test_samples = os.listdir(orig_test_dir)\n",
        "s = test_samples[:samples+1]\n",
        "\n",
        "for sample in s:\n",
        "\n",
        "  src = os.path.join(orig_test_dir , sample)\n",
        "  dest = os.path.join(test_dir , sample )\n",
        "  shutil.copyfile(src , dest)"
      ],
      "metadata": {
        "id": "79mRedGN3TkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_samples = os.listdir(train_dir)\n",
        "# note , for test it will return 0 , since the samples are not named after the animal\n",
        "val_samples = os.listdir(val_dir)\n",
        "test_samples = os.listdir(test_dir)\n",
        "\n",
        "print(\"number of training samples\" , len(train_samples))\n",
        "print(\"number of val samples\" , len(val_samples))\n",
        "print(\"number of test samples\" , len(test_samples))\n",
        "\n",
        "dogs = 0\n",
        "cats = 0\n",
        "\n",
        "for train in train_samples : #change the train_samples to val_samples\n",
        "\n",
        "  if train.startswith('dog'):\n",
        "    dogs+=1\n",
        "\n",
        "  if train.startswith('cat'):\n",
        "    cats+=1\n",
        "\n",
        "\n",
        "print(\"number of dogs samples\" , dogs)\n",
        "print(\"number of cats samples\" , cats)"
      ],
      "metadata": {
        "id": "bSYw_CnL8YRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "img = cv2.imread(os.path.join(train_dir , train_samples[10]))\n",
        "\n",
        "size = (250 , 250)\n",
        "\n",
        "img = cv2.resize(img , size)"
      ],
      "metadata": {
        "id": "JXxS01cNDF_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "cv2_imshow(img)"
      ],
      "metadata": {
        "id": "uJvQqT3jECjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_samples[0])"
      ],
      "metadata": {
        "id": "zzGM0ligH4rU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "base_train_dir = \"dataset/train\"\n",
        "base_val_dir = \"dataset/val\"\n",
        "base_test_dir = \"dataset/test\"\n",
        "\n",
        "dogs = []\n",
        "cats = []\n",
        "\n",
        "for m in os.listdir(base_val_dir):\n",
        "\n",
        "  if  m.startswith('dog'):\n",
        "    dogs.append(m)\n",
        "\n",
        "  if m.startswith('cat'):\n",
        "    cats.append(m)\n",
        "\n",
        "# train_dogs_dir = os.mkdir(\"dataset/train/dogs\")\n",
        "# train_cats_dir = os.mkdir(\"dataset/train/cats\")\n",
        "# val_dogs_dir = os.mkdir(\"dataset/val/dogs\")\n",
        "# val_cats_dir = os.mkdir(\"dataset/val/cats\")\n",
        "\n",
        "for i in dogs:\n",
        "  src = os.path.join(base_val_dir , i)\n",
        "  dest = \"dataset/val/dogs\"\n",
        "  shutil.move(src , dest)\n",
        "\n",
        "for i in cats:\n",
        "  src = os.path.join(base_val_dir , i)\n",
        "  dest = \"dataset/val/cats\"\n",
        "  shutil.move(src , dest)"
      ],
      "metadata": {
        "id": "8cmehFVPzjg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building Network"
      ],
      "metadata": {
        "id": "X4J0rSksCLa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(32  , (3,3) , activation = 'relu' , input_shape = (250 , 250 ,3)))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Conv2D(64 , (3,3) , activation = 'relu'))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Conv2D(128, (3,3) , activation = 'relu'))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Conv2D(128 , (3,3) , activation = 'relu'))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Conv2D(64 , (3,3) , activation = 'relu'))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(512 , activation = 'relu'))\n",
        "model.add(layers.Dense(1 , activation = 'sigmoid'))"
      ],
      "metadata": {
        "id": "LRtd9Sb_DB0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "rsLTXZjTu0Sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import optimizers\n",
        "\n",
        "model.compile(optimizer = optimizers.RMSprop(learning_rate = 1e-4) , loss = 'binary_crossentropy' , metrics = ['acc'] )"
      ],
      "metadata": {
        "id": "WqdCPtf0wKXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = 'dataset/train/'\n",
        "val_dir = 'dataset/val'\n",
        "test_dir = 'dataset/test'"
      ],
      "metadata": {
        "id": "LA29iYAzymz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DataLoading\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(train_dir ,\n",
        "                                                    target_size = (250,250) ,\n",
        "                                                    batch_size = 20 ,\n",
        "                                                    class_mode = 'binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(val_dir ,\n",
        "                                                        target_size = (250,250) ,\n",
        "                                                        batch_size = 25 ,\n",
        "                                                        class_mode = 'binary')"
      ],
      "metadata": {
        "id": "oXh9htKqwpW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing dataloader:\n",
        "\n",
        "for data_batch , labels_batch in train_generator :\n",
        "  print('data' , data_batch.shape)\n",
        "  print('labels' , labels_batch.shape)\n",
        "  break"
      ],
      "metadata": {
        "id": "UMTFQGktykNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "history = model.fit_generator(train_generator ,\n",
        "                              steps_per_epoch = 50 ,\n",
        "                              epochs = 10 ,\n",
        "                              validation_data = validation_generator ,\n",
        "                              validation_steps = 20)\n",
        "\n",
        "model.save('cats_and_dogs_small_1.h5')"
      ],
      "metadata": {
        "id": "URm7_Jg82s9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('model_history.json' , 'w') as f :\n",
        "  json.dump(history.history , f)"
      ],
      "metadata": {
        "id": "yIEyge01GNZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1 , len(acc)+1)\n",
        "\n",
        "plt.plot(epochs , acc , 'bo' , label = 'Training Acc')\n",
        "plt.plot(epochs , val_acc , 'b' , label = 'Val Acc')\n",
        "plt.title(\"Training vs Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs , loss , 'bo' , label = 'Training loss')\n",
        "plt.plot(epochs , val_loss , 'b' , label = 'Val loss')\n",
        "plt.title(\"Training vs Validation loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OPLBN9ls3crD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see clearly , We have overfitting problem"
      ],
      "metadata": {
        "id": "Rtslxq7mGhoG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding DropOut layer and Data Augmentation to increase dataset"
      ],
      "metadata": {
        "id": "KTSIRy9iG_pb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(32  , (3,3) , activation = 'relu' , input_shape = (250 , 250 ,3)))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Conv2D(64 , (3,3) , activation = 'relu'))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Conv2D(128, (3,3) , activation = 'relu'))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Conv2D(128 , (3,3) , activation = 'relu'))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Conv2D(64 , (3,3) , activation = 'relu'))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(512 , activation = 'relu'))\n",
        "model.add(layers.Dense(1 , activation = 'sigmoid'))\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "YRfW_anQIFgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Augmentation\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale = 1./255 ,\n",
        "    rotation_range = 40 ,\n",
        "    width_shift_range = 0.2 ,\n",
        "    height_shift_range = 0.2 ,\n",
        "    shear_range = 0.2 ,\n",
        "    zoom_range = 0.2 ,\n",
        "    horizontal_flip = True\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(train_dir ,\n",
        "                                                    target_size = (250,250) ,\n",
        "                                                    batch_size = 16,\n",
        "                                                    class_mode = 'binary')"
      ],
      "metadata": {
        "id": "OY53m5hgGfoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import optimizers\n",
        "\n",
        "model.compile(optimizer = optimizers.RMSprop(learning_rate = 1e-4) , loss = 'binary_crossentropy' , metrics = ['acc'] )"
      ],
      "metadata": {
        "id": "33ybANNSy1vL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit_generator(\n",
        "    train_generator ,\n",
        "    steps_per_epoch = 50,\n",
        "    epochs = 100 ,\n",
        "    validation_data = validation_generator ,\n",
        "    validation_steps = 25\n",
        ")\n",
        "\n",
        "model.save('cats_and_dogs_small_2.h5')"
      ],
      "metadata": {
        "id": "2b6az-kvIXgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('model_history_2.json' , 'w') as f :\n",
        "\n",
        "  json.dump(history.history , f)"
      ],
      "metadata": {
        "id": "cfUzXzzsIvmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1 , len(acc)+1)\n",
        "\n",
        "plt.plot(epochs , acc , 'bo' , label = 'Training Acc')\n",
        "plt.plot(epochs , val_acc , 'b' , label = 'Val Acc')\n",
        "plt.title(\"Training vs Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs , loss , 'bo' , label = 'Training loss')\n",
        "plt.plot(epochs , val_loss , 'b' , label = 'Val loss')\n",
        "plt.title(\"Training vs Validation loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Mmwf2VyUJC9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see the performance increased significantly compared to unaugmented model.\n",
        "\n",
        "By applying L1 or L2 regularization will enhance the results more."
      ],
      "metadata": {
        "id": "phdP1GICYeBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using a Pretrained Model"
      ],
      "metadata": {
        "id": "o9IorytYYuGe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For Feature Extraction\n",
        "\n",
        "will use pretrained [VGG16](https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg16/VGG16).\n",
        "[Architecture](https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.geeksforgeeks.org%2Fvgg-16-cnn-model%2F&psig=AOvVaw18JAs0D2qlc6YD2pWEy9WT&ust=1703869812399000&source=images&cd=vfe&opi=89978449&ved=0CBIQjRxqFwoTCKCkvevPsoMDFQAAAAAdAAAAABAI)\n"
      ],
      "metadata": {
        "id": "Fi0L6h4AY8kr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications import VGG16\n",
        "\n",
        "conv_base = VGG16(\n",
        "    include_top = False , # classifer\n",
        "    weights = 'imagenet' ,\n",
        "    input_shape = (250 , 250 , 3)\n",
        ")"
      ],
      "metadata": {
        "id": "pXK-Ik_eYm_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary()"
      ],
      "metadata": {
        "id": "HeMEoJMPbLJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256 , activation = 'relu'))\n",
        "model.add(layers.Dense(1 , activation = 'sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "P_zAE24IbUl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import optimizers\n",
        "\n",
        "model.compile(optimizer = optimizers.RMSprop(learning_rate = 2e-5) , loss = 'binary_crossentropy' , metrics = ['acc'])"
      ],
      "metadata": {
        "id": "S-2x-ZD6ds2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Needs GPU to run it\n",
        "\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator ,\n",
        "    steps_per_epoch = 50 ,\n",
        "    epochs = 15,\n",
        "    validation_data = validation_generator ,\n",
        "    validation_steps = 25\n",
        ")"
      ],
      "metadata": {
        "id": "hlTyRSIJdbOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1 , len(acc)+1)\n",
        "\n",
        "plt.plot(epochs , acc , 'bo' , label = 'Training Acc')\n",
        "plt.plot(epochs , val_acc , 'b' , label = 'Val Acc')\n",
        "plt.title(\"Training vs Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs , loss , 'bo' , label = 'Training loss')\n",
        "plt.plot(epochs , val_loss , 'b' , label = 'Val loss')\n",
        "plt.title(\"Training vs Validation loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "d90RmHrpeWD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FineTuning"
      ],
      "metadata": {
        "id": "sEps6a-BegUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.trainable = True\n",
        "\n",
        "set_trainable = False\n",
        "\n",
        "for layer in conv_base.layers:\n",
        "  if layer == 'block5_conv1':\n",
        "    set_trainable = True\n",
        "\n",
        "  if set_trainable:\n",
        "    layer.trainable = True\n",
        "\n",
        "  else :\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "JFDxNfXgeiF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss = 'binary_crossentropy' , optimizer = optimizers.RMSprop(learning_rate = 1e-5) , metrics = ['acc'])"
      ],
      "metadata": {
        "id": "HWgSWhLIfgrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#needs GPU\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator ,\n",
        "    steps_per_epoch = 50 ,\n",
        "    epochs = 100 ,\n",
        "    validation_data = validation_generator ,\n",
        "    validation_steps = 50\n",
        ")"
      ],
      "metadata": {
        "id": "xgJg6_47fxw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir ,\n",
        "    target_size = (250 ,250) ,\n",
        "    batch_size = 20,\n",
        "    class_mode = 'binary'\n",
        ")\n",
        "\n",
        "test_loss , test_acc = model.evaluate_generator(test_generator , steps = 50)\n",
        "print(\"test_acc :\" , test_acc)"
      ],
      "metadata": {
        "id": "y7z6WvYJgDkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z9LLNFTh9zo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to tensorflow/keras"
      ],
      "metadata": {
        "id": "yYThv5Vj9rcM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a Simple Classifier"
      ],
      "metadata": {
        "id": "jemtKB1mNnBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "num_samples_per_class = 1000\n",
        "\n",
        "# Generate the first class of points\n",
        "negative_samples = np.random.multivariate_normal(\n",
        "    mean = [0 , 3] ,\n",
        "    cov = [[1 , 0.5] , [0.5 , 1]], # this corresponds to oval-like point cloud oriented from bottom left to top right\n",
        "    size = num_samples_per_class\n",
        ")\n",
        "\n",
        "positive_samples = np.random.multivariate_normal(\n",
        "    mean = [3 , 0] ,\n",
        "    cov = [[1 , 0.5] , [0.5 , 1]],\n",
        "    size = num_samples_per_class\n",
        ")\n",
        "\n",
        "inputs = np.vstack((negative_samples , positive_samples)).astype(np.float32) # stacking the two classes in an array of shape (2000 , 2)\n",
        "\n",
        "targets = np.vstack((np.zeros((num_samples_per_class , 1) , dtype = \"float32\") , np.ones((num_samples_per_class , 1) , dtype = 'float32')))"
      ],
      "metadata": {
        "id": "INzjkkENKT3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "input_dim = 2\n",
        "output_dim = 1\n",
        "\n",
        "W = tf.Variable(initial_value = tf.random.uniform(shape = (input_dim , output_dim))) #can be modified\n",
        "b = tf.Variable(initial_value= tf.zeros(shape = (output_dim ,)))"
      ],
      "metadata": {
        "id": "UaA5-yMyGB4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Forward Pass\n",
        "\n",
        "def model(inputs):\n",
        "  return tf.matmul(inputs , W) + b\n",
        "\n",
        "#Loss Function\n",
        "\n",
        "def square_loss(targets , predicitions):\n",
        "  per_sample_loss = tf.square(targets - predicitions)\n",
        "  return tf.reduce_mean(per_sample_loss)\n",
        "\n",
        "lr = 0.1\n",
        "\n",
        "def training_step(inputs , targets):\n",
        "  with tf.GradientTape() as tape: #This line sets up a context in which TensorFlow will record operations for the forward pass to compute gradients later.\n",
        "    # Forward pass\n",
        "    predicitions = model(inputs)\n",
        "    loss = square_loss(targets , predicitions)\n",
        "  #Retrieve the gradient of the loss with regard to weights\n",
        "  grad_loss_wrt_W , grad_loss_wrt_b = tape.gradient(loss , [W,b])\n",
        "  # update the weights\n",
        "  W.assign_sub(grad_loss_wrt_W * lr)\n",
        "  b.assign_sub(grad_loss_wrt_b * lr)\n",
        "  return loss"
      ],
      "metadata": {
        "id": "IDCOzY4xGq8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for step in range(40):\n",
        "  loss = training_step(inputs , targets)\n",
        "  print(f\"Loss at step{step}: {loss:.4f}\")"
      ],
      "metadata": {
        "id": "myjJl1H4IKtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "predicitions = model(inputs)\n",
        "plt.scatter(inputs[: , 0] , inputs[: , 1] , c = predicitions[: , 0] > 0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YWio9bBEL6Jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.linspace(-1 , 4 , 100) # generate 100 regulary spaced numbers between -1 and 4\n",
        "\n",
        "y = - W[0] / W[1] * x + (0.5 - b) / W[1]\n",
        "plt.plot(x , y , \"-r\")\n",
        "plt.scatter(inputs[: , 0] , inputs[: , 1] , c = predicitions[: , 0] > 0.5)"
      ],
      "metadata": {
        "id": "CfvIYRfbMV1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Block of Deep Learning"
      ],
      "metadata": {
        "id": "K-NOSemoNsDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "class SimpleDense(keras.layers.Layer): #all keras layers inherit from the base Layer class\n",
        "\n",
        "  def __init__(self , units , activation = None):\n",
        "    super().__init__()\n",
        "    self.units = units\n",
        "    self.activation = activation\n",
        "\n",
        "  # weight creation takes place in the build() method\n",
        "  def build(self , input_shape):\n",
        "    input_dim = input_shape[-1]\n",
        "    '''\n",
        "    add_weight() is a shortcut method for creating weights , we can use tf.Variable like before.\n",
        "\n",
        "     '''\n",
        "    self.W = self.add_weight(shape = (input_dim , self.units) , initializer = \"random_normal\")\n",
        "    self.b = self.add_weight(shape = (self.units) , initializer = \"zeros\")\n",
        "\n",
        "  # forward pass is defined in the call functions\n",
        "  def call(self , inputs):\n",
        "    y = tf.matmul(inputs , self.W) + self.b\n",
        "    if self.activation is not None:\n",
        "      y = self.activation(y)\n",
        "\n",
        "    return y"
      ],
      "metadata": {
        "id": "D5f0PB0tNwl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_dense = SimpleDense(units = 32 , activation = tf.nn.relu)\n",
        "\n",
        "input_tensor = tf.ones(shape = (2 , 784))\n",
        "output_tensor = my_dense(input_tensor)\n",
        "print(output_tensor.shape)"
      ],
      "metadata": {
        "id": "9rCNf8yPwL2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this is the same as the follows:\n",
        "\n",
        "model = keras.Sequential([\n",
        "\n",
        "    layers.Dense(32 , activation = \"relu\") ,\n",
        "    layers.Dense(32)\n",
        "]"
      ],
      "metadata": {
        "id": "IjQDbV-nwzOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Deep Learning for Computer Vision"
      ],
      "metadata": {
        "id": "7soJ0fNs32Ts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Segmentation\n",
        "\n",
        "In this example , we will focus on ** semantic segmentation **\n",
        "\n",
        "dataset : https://www.robots.ox.ac.uk/~vgg/data/pets/"
      ],
      "metadata": {
        "id": "ArTu8mmC4ycp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Download"
      ],
      "metadata": {
        "id": "LTEQtUBWC8QV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://thor.robots.ox.ac.uk/~vgg/data/pets/images.tar.gz\n",
        "!wget https://thor.robots.ox.ac.uk/~vgg/data/pets/annotations.tar.gz"
      ],
      "metadata": {
        "id": "yLOxaJSq37DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "import os\n",
        "\n",
        "# Replace 'your-file.tar.gz' with the path to your .tar.gz file\n",
        "file_path = '/content/images.tar.gz'\n",
        "file_path_2 = '/content/annotations.tar.gz'\n",
        "\n",
        "# Define the directory where you want to extract the files. It can be '.' to extract in the current directory.\n",
        "extract_dir = '/content/images'\n",
        "extract_dir_2 = '/content/annotations'\n",
        "\n",
        "# Make sure the extract directory exists\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "# Open the .tar.gz file and extract it\n",
        "with tarfile.open(file_path, 'r:gz') as tar:\n",
        "    tar.extractall(path=extract_dir)\n",
        "\n",
        "print(f\"Extracted all contents of {file_path} to {extract_dir}\")\n"
      ],
      "metadata": {
        "id": "63QoVrIO5W0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "images_dir = \"/content/images/images\"\n",
        "annotations_dir = \"/content/annotations/annotations/trimaps\"\n",
        "\n",
        "input_images = []\n",
        "img_files = sorted(os.listdir(images_dir))\n",
        "for files in img_files:\n",
        "  if files.endswith('.jpg'):\n",
        "    input_images.append(os.path.join(images_dir , files))\n",
        "\n",
        "\n",
        "#More advanced way to write it (more professional):\n",
        "\n",
        "#input_images = sorted ([\n",
        " # os.path.join(images_dir , fname) for fname in os.listdir(images_dir) if fname.endswith(\".jpg\")\n",
        "#])"
      ],
      "metadata": {
        "id": "jM4O-HrN5fSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_images[:10]"
      ],
      "metadata": {
        "id": "2Fj4wR8A7Hgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_paths = sorted([\n",
        "    os.path.join(annotations_dir , anno) for anno in os.listdir(annotations_dir) if anno.endswith(\".png\") and not anno.startswith(\".\") ])"
      ],
      "metadata": {
        "id": "fgnTS1qq8NIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_paths[:10]"
      ],
      "metadata": {
        "id": "TgNk0O8Q8ldd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import load_img , img_to_array\n",
        "\n",
        "index = 50\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(load_img(input_images[index]))"
      ],
      "metadata": {
        "id": "8qCdxl-o8qg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#annotations are pixel level trimap segmentation --> contains only 3 labels (1 , 2, 3)\n",
        "\n",
        "def display_target(target_array):\n",
        "  normalized_array = (target_array.astype(\"uint8\") - 1) * 127 # subtract by 1 so that the labels range between 0 to 2.\n",
        "  plt.axis(\"off\")\n",
        "  plt.imshow(normalized_array[: , :  , 0])\n",
        "\n",
        "img = img_to_array(load_img(target_paths[index] , color_mode = \"grayscale\"))\n",
        "display_target(img)"
      ],
      "metadata": {
        "id": "T0T2zyk28__C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### dataset splitting"
      ],
      "metadata": {
        "id": "5dvUT348DDOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "img_size = (200 , 200) # will resize everything to img_size\n",
        "\n",
        "num_imgs = len(input_images)\n",
        "print(\"total number of samples\" , num_imgs)\n",
        "random.Random(1337).shuffle(input_images)\n",
        "random.Random(1337).shuffle(target_paths)\n",
        "\n",
        "def path2image(image_path):\n",
        "  return img_to_array(load_img(image_path , target_size = img_size))\n",
        "\n",
        "def path2target(target_path):\n",
        "  img = img_to_array(load_img(target_path , target_size = img_size , color_mode = 'grayscale')) # channel dim will be 1\n",
        "  img = img.astype(\"uint8\") - 1\n",
        "  return img\n",
        "\n",
        "input_imgs = np.zeros((num_imgs , ) + img_size + (3 , ) , dtype = \"float32\") # images are RGB\n",
        "targets = np.zeros((num_imgs ,) + img_size + (1,) , dtype = \"uint8\") # targets are grayscale\n",
        "for i in range(num_imgs):\n",
        "  input_imgs[i] = path2image(input_images[i])\n",
        "  targets[i] = path2target(target_paths[i])\n",
        "\n",
        "num_val = 1000\n",
        "\n",
        "train_input_imgs = input_imgs[: -num_val]\n",
        "train_targets = targets[: -num_val]\n",
        "val_input_imgs = input_imgs[-num_val:]\n",
        "val_targets = targets[-num_val:]"
      ],
      "metadata": {
        "id": "ip0UhEmO9uyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Creation"
      ],
      "metadata": {
        "id": "wDmyP3NpDFE3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of the first half of this model is to encode the images into smaller feature maps , where each spatial location (pixel) contains information about a large spatial chunk of the image. In other words, COMPRESSION.\n",
        "\n",
        "Difference between this and the model we did for image classification , is that we used MaxPooling layers while here we used strides to downsample. This is because , in image segmentation , we care a lot about the spatial location of information in the image since we need to produce per pixel target mask as output of the model.\n",
        "\n",
        "2x2 maxpooling destroys completely the location information within each pooling window --> return one scaler value per window , with zero knowledge of which of the four locations in the windows the value came from.\n"
      ],
      "metadata": {
        "id": "-EB8CWdhL0pl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Model , layers\n",
        "\n",
        "def get_model(img_size , num_classes):\n",
        "\n",
        "  inputs = keras.Input(shape = img_size + (3,)) # (200 ,200) + (3,) for RGB --> (200 , 200 , 3)\n",
        "  x = layers.Rescaling(1./255)(inputs) # range between 0 and 1\n",
        "  #upsample\n",
        "  x = layers.Conv2D(64 , 3 ,  strides = 2 , activation = 'relu' , padding = 'same')(x)\n",
        "  x = layers.Conv2D(64 , 3 ,   activation = 'relu' , padding = 'same')(x)\n",
        "  x = layers.Conv2D(128 , 3 ,  strides = 2 , activation = 'relu' , padding = 'same')(x)\n",
        "  x = layers.Conv2D(128, 3 ,   activation = 'relu' , padding = 'same')(x)\n",
        "  x = layers.Conv2D(256 , 3 ,  strides = 2 , activation = 'relu' , padding = 'same')(x)\n",
        "  x = layers.Conv2D(256 , 3 , activation = 'relu' , padding = 'same')(x)\n",
        "  #downsample\n",
        "  x = layers.Conv2DTranspose(256 , 3 , activation = 'relu' , padding = 'same' )(x)\n",
        "  x = layers.Conv2DTranspose(256 , 3 , activation = 'relu' , padding = 'same' , strides = 2 )(x)\n",
        "  x = layers.Conv2DTranspose(128 , 3 , activation = 'relu' , padding = 'same' )(x)\n",
        "  x = layers.Conv2DTranspose(128 , 3 , activation = 'relu' , padding = 'same' , strides = 2)(x)\n",
        "  x = layers.Conv2DTranspose(64 , 3 , activation = 'relu' , padding = 'same' )(x)\n",
        "  x = layers.Conv2DTranspose(64 , 3 , activation = 'relu' , padding = 'same' , strides = 2 )(x)\n",
        "\n",
        "  outputs = layers.Conv2D(num_classes , 3 , activation = \"softmax\" , padding = 'same')(x)\n",
        "\n",
        "  model = keras.Model(inputs , outputs)\n",
        "  return model\n",
        "\n",
        "model = get_model(img_size = (200,200) , num_classes = 3)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "QvA3HNlMB1C3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = 'rmsprop' , loss = 'sparse_categorical_crossentropy')\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"oxford_segmentation.keras\" ,\n",
        "                                    save_best_only = True)\n",
        "]\n",
        "\n",
        "history = model.fit(train_input_imgs ,\n",
        "                    train_targets ,\n",
        "                    epochs = 15 ,\n",
        "                    callbacks = callbacks ,\n",
        "                    batch_size = 64 ,\n",
        "                    validation_data = (val_input_imgs , val_targets))"
      ],
      "metadata": {
        "id": "DllZ4X2cJ82j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1 , len(history.history[\"loss\"]) + 1)\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "plt.figure()\n",
        "plt.plot(epochs , loss , \"bo\" , label = \"Training loss\")\n",
        "plt.plot(epochs , val_loss , \"b\" , label = \"Val loss\")\n",
        "plt.title(\"training vs val loss\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "JOBll4oMQSZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import array_to_img\n",
        "\n",
        "model = keras.models.load_model(\"/content/oxford_segmentation.keras\")\n",
        "\n",
        "i = 4\n",
        "test_image = val_input_imgs[i]\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(array_to_img(test_image))\n",
        "mask = model.predict(np.expand_dims(test_image , 0))[0]\n",
        "\n",
        "def display_mask(pred):\n",
        "  mask = np.argmax(pred , axis = -1)\n",
        "  mask*=127\n",
        "  plt.axis(\"off\")\n",
        "  plt.imshow(mask)\n",
        "\n",
        "display_mask(mask)"
      ],
      "metadata": {
        "id": "9ec4xIJ1hLZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**It needs more training !**"
      ],
      "metadata": {
        "id": "VA9BUxxeh7eX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning For Text"
      ],
      "metadata": {
        "id": "IUBQh_G-VOSQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PreProcessing Text"
      ],
      "metadata": {
        "id": "LHL-IEsHb-rx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### from scratch using simple python"
      ],
      "metadata": {
        "id": "xkLzL6vPi458"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "class Vectorizer:\n",
        "\n",
        "  def standarize(self , text):\n",
        "    text = text.lower()\n",
        "    return \" \".join(char for char in text\n",
        "                   if char not in string.punctuation)\n",
        "\n",
        "\n",
        "  def tokenize(self , text):\n",
        "    text = self.standarize(text)\n",
        "    return text.split()\n",
        "\n",
        "  def make_vocabulary(self , dataset):\n",
        "\n",
        "    self.vocabulary = {\" \" : 0 , \"[UNK]\" : 1}\n",
        "    #The empty string \"\" and a special token \"[UNK]\" (representing unknown words) are initialized with indices 0 and 1, respectively. The unknown token \"[UNK]\" is used to represent words that are not found in the vocabulary.\n",
        "    for text in dataset:\n",
        "      text = self.standarize(text)\n",
        "      tokens = self.tokenize(text)\n",
        "      for token in tokens:\n",
        "        if token not in self.vocabulary:\n",
        "          self.vocabulary[token] = len(self.vocabulary) # for example ,if immediately we want to add a token , it will add token : 2 (where 2 is the length of the vocabulary before adding)\n",
        "    self.inverse_vocabulary = dict(\n",
        "        (v , k) for k ,v in self.vocabulary.items()\n",
        "    )\n",
        "\n",
        "\n",
        "  def encode(self , text):\n",
        "    text = self.standarize(text)\n",
        "    tokens = self.tokenize(text)\n",
        "    return [self.vocabulary.get(token , 1) for token in tokens ]\n",
        "    # This part of the code gets the index of token from self.vocabulary. If the token does not exist in the vocabulary, it returns 1. The number 1 is chosen because, in your vocabulary initialization, 1 is the index for the special \"[UNK]\" token,\n",
        "    # which stands for \"unknown\". This token is used to represent words that are not found in the vocabulary.\n",
        "    #  .get() method is that it allows you to specify a default value to return if the specified key is not found in the dictionary.\n",
        "    # dictionary.get(key, default_value)\n",
        "\n",
        "  def decode(self , int_sequence):\n",
        "    return \"\".join(\n",
        "        self.inverse_vocabulary.get(i , \"[UNK]\" )for i in int_sequence)"
      ],
      "metadata": {
        "id": "-wt8Q9MHh15Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = Vectorizer()\n",
        "\n",
        "dataset = [\n",
        "    \"I write , erase , rewrite\" ,\n",
        "    \"Erase again , and then\" ,\n",
        "    \"A poppy blooms\"\n",
        "]"
      ],
      "metadata": {
        "id": "uvTpN2Pihe3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer.make_vocabulary(dataset)"
      ],
      "metadata": {
        "id": "7sF31WoYhywC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = \"I write , rewrite , and still rewrite again\"\n",
        "encoded_sentence = vectorizer.encode(test_sentence)\n",
        "print(encoded_sentence)\n",
        "\n",
        "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
        "print(decoded_sentence)"
      ],
      "metadata": {
        "id": "w3WdhFb7iBQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code works , but not efficient"
      ],
      "metadata": {
        "id": "KLEusMOBjRN-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using built-in functions"
      ],
      "metadata": {
        "id": "-oPKiR_wjOxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode = 'int'\n",
        ")\n",
        "# by default it will use the .lower and the remove puncutation and split on whitespace.\n",
        "\n",
        "dataset = text_vectorization.adapt(dataset)"
      ],
      "metadata": {
        "id": "1-eca9QRioX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"vocabulary:\" , text_vectorization.get_vocabulary())\n",
        "\n",
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "test_sentence = \"I write , rewrite , and still rewrite again\"\n",
        "\n",
        "encoded_sentence = text_vectorization(test_sentence)\n",
        "print(\"encoded : \" , encoded_sentence)\n",
        "inverse_vocab = dict(enumerate(vocabulary))\n",
        "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
        "print(\"decoded:\" , decoded_sentence)"
      ],
      "metadata": {
        "id": "yihcGS05kip5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Non-Sequential Models"
      ],
      "metadata": {
        "id": "u8BiHdgFt1TZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf /content/aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "id": "yEkJnireDw_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/aclImdb/train/unsup"
      ],
      "metadata": {
        "id": "9MtmqB7FD3ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/aclImdb/train/pos/10000_8.txt"
      ],
      "metadata": {
        "id": "RaflVEWJEkk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating Validation set\n",
        "\n",
        "import os\n",
        "import random\n",
        "import shutil , pathlib\n",
        "\n",
        "os.mkdir(\"/content/aclImdb/val\")\n",
        "base_dir = pathlib.Path(\"/content/aclImdb\")\n",
        "train_dir = base_dir / \"train\"\n",
        "val_dir = base_dir / \"val\"\n",
        "\n",
        "for category in (\"neg\" , \"pos\"):\n",
        "  os.mkdir(val_dir/category)\n",
        "  files = os.listdir(train_dir/category)\n",
        "  print(f\"number of {category} samples before split\" , len(files))\n",
        "  random.Random(1337).shuffle(files)\n",
        "  num_val_samples =int( 0.2 * len(files))\n",
        "  val_files = files[-num_val_samples:]\n",
        "  print(f\"number of {category} samples after split \")\n",
        "  for sample in val_files:\n",
        "    shutil.move(train_dir/category/sample ,\n",
        "                val_dir/category/sample)\n"
      ],
      "metadata": {
        "id": "1rQgNFrYErz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\"/content/aclImdb/train\" , batch_size = batch_size) # used to create a batched dataset\n",
        "val_ds = keras.utils.text_dataset_from_directory(\"/content/aclImdb/val\" , batch_size = batch_size)\n",
        "test_ds = keras.utils.text_dataset_from_directory(\"/content/aclImdb/test\" , batch_size = batch_size)"
      ],
      "metadata": {
        "id": "2wBBJgJMIHBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Text Vectorization\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "#limit the vocabulary to the 20,000 most frequently used words in the data, otherwise we would be indexing every word in the training data.\n",
        "text_vectorization = TextVectorization(\n",
        "    #ngrams = None --> unigram 2 --> bigrams\n",
        "    max_tokens = 20000 ,\n",
        "    output_mode = 'multi_hot', # multi-hot is a binary encoding of multiple tokens in a single vector. setting it to 'count' will return number of times the ngram occured\n",
        "    # setting it to \"tf_idf\" --> weights a given term by taking \"term freq\" , how many times the term appears in the current document and dividing it by a measure of \"document frequency\" which estimates how much the term appears across the dataset\n",
        ")\n",
        "\n",
        "text_only_train_ds = train_ds.map(lambda x , y:x) #  takes two arguments, x and y. The function returns only the first argument x --> raw text inputs with no labels\n",
        "text_vectorization.adapt(text_only_train_ds) # use that dataset to index the dataset vocabulary\n",
        "\n",
        "#prepare processed versions of train,test and val sets\n",
        "binary_lgram_train_ds = train_ds.map(\n",
        "    lambda x,y: (text_vectorization(x) , y)  ,\n",
        "    num_parallel_calls = 4\n",
        ")\n",
        "\n",
        "binary_lgram_val_ds = val_ds.map(\n",
        "    lambda x,y : (text_vectorization(x) , y)  ,\n",
        "    num_parallel_calls = 4\n",
        ")\n",
        "\n",
        "binary_lgram_test_ds = test_ds.map(\n",
        "    lambda x,y : (text_vectorization(x) , y)  ,\n",
        "    num_parallel_calls = 4 # leverage multiple CPU cores , since the TextVectorization doesn't work on GPU\n",
        ")"
      ],
      "metadata": {
        "id": "r2HantVgI4l7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model(max_tokens = 20000 , hidden_dim = 6):\n",
        "\n",
        "  inputs = keras.Input(shape = (max_tokens , ))\n",
        "  x = layers.Dense(hidden_dim , activation = 'relu')(inputs)\n",
        "  x = layers.Dropout(0.5)(x)\n",
        "  outputs = layers.Dense(1 , activation = \"sigmoid\")(x)\n",
        "  model = keras.Model(inputs , outputs)\n",
        "  model.compile(optimizer = 'rmsprop' ,\n",
        "                loss = \"binary_crossentropy\" ,\n",
        "                metrics = [\"accuracy\"])\n",
        "  return model"
      ],
      "metadata": {
        "id": "Q9WuReAyPGY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "UuSddl1VLClk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_lgram.keras\" ,\n",
        "                                    save_best_only = True)\n",
        "]\n",
        "\n",
        "model.fit(binary_lgram_train_ds.cache() , # we call cache on the dataset to cache it to memory\n",
        "          validation_data = binary_lgram_val_ds ,\n",
        "          epochs = 10 ,\n",
        "          callbacks = callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"binary_lgram.keras\")\n",
        "print(f\"test accuracy : {model.evaluate(binary_lgram_test_ds)[1] : .3f}\")"
      ],
      "metadata": {
        "id": "nx--qUmuLDhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sequential Models"
      ],
      "metadata": {
        "id": "vbpnmEWzt6A-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### bidirectional RNNs(bidirectional LSTMs)"
      ],
      "metadata": {
        "id": "kNqSbZcaVlDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "max_length = 600 #max length of a token\n",
        "max_tokens = 20000 #vocab size\n",
        "\n",
        "\n",
        "#vocab layer\n",
        "text_vectorization = layers.TextVectorization(      #A preprocessing layer which maps text features to integer sequences.\n",
        "    max_tokens = max_tokens ,\n",
        "    output_mode = 'int' ,\n",
        "    output_sequence_length = max_length\n",
        ")\n",
        "\n",
        "#The vocabulary for the layer must be either supplied on construction or learned via adapt()\n",
        "\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(        # map() applies the lambda function to each element of the train_ds\n",
        "    lambda x,y: (text_vectorization(x) , y) ,\n",
        "    num_parallel_calls = 4)\n",
        "\n",
        "int_val_ds = val_ds.map(\n",
        "    lambda x,y: (text_vectorization(x) , y) ,\n",
        "    num_parallel_calls = 4\n",
        ")\n",
        "\n",
        "int_test_ds = test_ds.map(\n",
        "    lambda x,y: (text_vectorization(x) , y),\n",
        "    num_parallel_calls = 4\n",
        ")"
      ],
      "metadata": {
        "id": "ZdNW2wW8rqxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now we need to transfrom these integer sequences to vector sequences via hot encoding.\n",
        "\n",
        "each dimension would represent one possible term in the vocabulary."
      ],
      "metadata": {
        "id": "Lv2joH-fcYqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "#one input is a sequence of integers\n",
        "inputs = keras.Input(shape = (None , ) , dtype = \"int64\") # None --> unspecified number along first dimension (not constant) --> this shape in keras.Inputs mean that it can accept any length along this axis /\n",
        "#we specified the dtype because the tf.one_hot requires the input in a specific type\n",
        "\n",
        "embedd = tf.one_hot(inputs , depth = max_tokens ) # Encode the integers into a binary 20,000 dimensional vectors\n",
        "\n",
        "x = layers.Bidirectional(layers.LSTM(units = 32))(embedd) # for sequential processing\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1 , activation = \"sigmoid\")(x) #classification layer\n",
        "model = keras.Model(inputs , outputs)\n",
        "\n",
        "model.compile(optimizer = 'rmsprop' ,\n",
        "              loss = 'binary_crossentropy' ,\n",
        "              metrics = ['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "g2ynvi9gWrxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\" ,\n",
        "                                    save_best_only = True)\n",
        "]\n",
        "\n",
        "model.fit(int_train_ds ,\n",
        "          validation_data = int_val_ds ,\n",
        "          epochs = 10 ,\n",
        "          callbacks = callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"content/one_hot_bidir_lstm.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "id": "FP0BwIahd2LH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model trains slowly , and thats because the inputs are large.\n",
        "\n",
        "each input is encoded as a matrix of size (600 , 20000) --> 600 words per sample , 200000 possible words\n",
        "\n",
        "Even that , the evaluations are not that high compared to our previously used network.\n",
        "\n",
        "lets explore **Word Embeddings**"
      ],
      "metadata": {
        "id": "5qwyCE7Hj9wu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "max_tokens = 600\n",
        "embedding_layer = layers.Embedding(input_dim = max_tokens , output_dim = 256) # best understood as dictionary that maps integer indices (which stands for specific words) to dense vectors. --> dictionary lookup.\n",
        "# weights initially random , during training these word vectors are updated via backpropagation\n",
        "# word index --> Embedding layer --> Corresponding word vector\n",
        "# input (batch_size , sequence_length) where each entry is a sequence of integers\n",
        "# layer returns a 3D floating-point tensor of shape (batch_size , sequence_length , embedding_dimensionality)"
      ],
      "metadata": {
        "id": "o7jcvwByjaY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model that uses Embedding layer\n",
        "\n",
        "inputs = keras.Input(shape = (None , ) , dtype = \"int64\")\n",
        "embedd = layers.Embedding(input_dim = max_tokens , output_dim = 256)(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedd)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1 , activation = \"sigmoid\")(x)\n",
        "model = keras.Model(inputs , outputs)\n",
        "\n",
        "model.compile(optimizer = 'rmsprop' , loss = 'binary_crossentropy' , metrics = ['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "-fpEbDsCppgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"embedding_bidir_gru.keras\" ,\n",
        "    save_best_only = True)\n",
        "]\n",
        "\n",
        "model.fit(int_train_ds ,\n",
        "          validation_data = int_val_ds ,\n",
        "          epochs = 10 ,\n",
        "          callbacks = callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"/content/embedding_bidir_gru.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1] :.3f}\")"
      ],
      "metadata": {
        "id": "F1SVKUxbqby1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model trains much faster than one-hot model!\n",
        "\n",
        "but still the bigram model is better in terms of accuracy on test set.\n",
        "\n",
        "one reason is that the bigram model processed full reviews , while our sequence model truncates sequences to 600 words\n",
        "\n",
        "Lets explore **Padding and Masking**"
      ],
      "metadata": {
        "id": "NmbqdoNGrYOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here , what is hurting the performance , is that sentences longer than 600 tokens are truncated to a length of 600 tokens , while sentences shorter than 600 (max_length) are padded with zeros at the end."
      ],
      "metadata": {
        "id": "4p1MXNadawMT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bidirectional RNN --> Using two RNN layers running in parallel --> on processing the tokens in their natural order , and the second processing the same tokens in reverse.\n",
        "\n",
        "The RNN seeing the tokens in its natural order will spend hundreds of iterations if the original sentence is short due to padded zeros.\n",
        "\n",
        "The information stored in the internal state of the RNN will gradually fade out as it gets exposed to these meaningless inputs.\n",
        "\n",
        "Therefore we need a way to tell the RNN to skip these iterations.\n",
        "\n",
        "There is an API for that called masking."
      ],
      "metadata": {
        "id": "Std1YhV1b65d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = keras.layers.Embedding(input_dim = 10 , output_dim = 256 , mask_zero = True)\n",
        "some_input = [[4 , 3 , 2 , 1 , 0 , 0 ,0 ],\n",
        "              [5 , 4 , 3 , 2 , 1 , 0 ,0] ,\n",
        "              [2 , 1 , 0 , 0 , 0, 0 , 0]]\n",
        "\n",
        "mask = embedding_layer.compute_mask(some_input)\n",
        "mask"
      ],
      "metadata": {
        "id": "ZMn2xjiprPnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using and embedding layer with masking enabled\n",
        "\n",
        "inputs = keras.Input(shape = (None , ) , dtype = 'int64')\n",
        "embedd = keras.layers.Embedding(\n",
        "    input_dim = max_tokens , output_dim = 256 , mask_zero = True )(inputs)\n",
        "\n",
        "x = keras.layers.Bidirectional(keras.layers.LSTM(32))(embedd)\n",
        "x = keras.layers.Dropout(0.5)(x)\n",
        "outputs = keras.layers.Dense(1 , activation = \"sigmoid\")(x)\n",
        "model = keras.Model(inputs , outputs)\n",
        "\n",
        "model.compile(optimizer = 'rmsprop' , loss = 'binary_crossentropy' , metrics = ['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "8QRl9JjXi22u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru_with_masking.keras\" ,\n",
        "                                    save_best_only = True )\n",
        "]\n",
        "\n",
        "model.fit(int_train_ds ,\n",
        "          validation_data = int_val_ds ,\n",
        "          epochs = 10 ,\n",
        "          callbacks = callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"/content/embeddings_bidir_gru_with_masking.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1] :.3f}\")"
      ],
      "metadata": {
        "id": "t3PcLKtZku0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A small noticeable improvement"
      ],
      "metadata": {
        "id": "MdPlZQl6lhBW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pretrained Word Embeddings\n",
        "\n",
        "popular ones : word2vec , glove"
      ],
      "metadata": {
        "id": "t7859W32luUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "metadata": {
        "id": "k_yMMh-GlZdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#index that maps words\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "path_to_glove = \"/content/glove.6B.100d.txt\"\n",
        "\n",
        "embedding_index = {}\n",
        "\n",
        "with open(path_to_glove) as f:\n",
        "  for line in f:\n",
        "    word , coefs = line.split(maxsplit = 1)\n",
        "    coefs = np.fromstring(coefs , \"f\" , sep = \" \")\n",
        "    embedding_index[word] = coefs\n",
        "\n",
        "print(f\"Found {len(embedding_index)} word vectors\")"
      ],
      "metadata": {
        "id": "x0gwWh-GmTHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating an embedding matrix to be loaded to the embedding layer\n",
        "\n",
        "embedding_dim = 100\n",
        "vocabulary = text_vectorization.get_vocabulary() # Retrieve the vocabulary indexed by our previous TextVectorization layer\n",
        "word_index = dict(zip(vocabulary , range(len(vocabulary)))) # used to create a mapping from words to their index in the vocabulary\n",
        "\n",
        "embedding_matrix = np.zeros((max_tokens , embedding_dim)) # prepare a matrix that we'll fill with the glove vectors.\n",
        "for word , i in word_index.items():\n",
        "  if i> max_tokens:\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      embedding_matrix[i] = embedding_vector # fill entry i int the matrix with the word vector for index i , words not found in the embedding will be all zeros"
      ],
      "metadata": {
        "id": "aeY1jyoVnnpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = keras.layers.Embedding(\n",
        "    max_tokens ,\n",
        "    embedding_dim  ,\n",
        "    embeddings_initializer = keras.initializers.Constant(embedding_matrix) ,  # load pre-trained embeddings\n",
        "    trainable = False ,  #freeze layer\n",
        "    mask_zero = True\n",
        ")"
      ],
      "metadata": {
        "id": "Y6NMox8gottG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model that uses pre-trained embeddings\n",
        "\n",
        "inputs = keras.Input(shape = (None , ) , dtype = 'int64')\n",
        "embedd = embedding_layer(inputs)\n",
        "\n",
        "x = keras.layers.Bidirectional(keras.layers.LSTM(32))(embedd)\n",
        "x = keras.layers.Dropout(0.5)(x)\n",
        "outputs = keras.layers.Dense(1 , activation = \"sigmoid\")(x)\n",
        "model = keras.Model(inputs , outputs)\n",
        "\n",
        "model.compile(optimizer = 'rmsprop' , loss = 'binary_crossentropy' , metrics = ['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "M4TVVt7vpU7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"glove_embeddings.keras\" ,\n",
        "                                    save_best_only = True )\n",
        "]\n",
        "\n",
        "model.fit(int_train_ds ,\n",
        "          validation_data = int_val_ds ,\n",
        "          epochs = 10 ,\n",
        "          callbacks = callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"/content/glove_embeddings.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1] :.3f}\")"
      ],
      "metadata": {
        "id": "GwI240AoptPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformers"
      ],
      "metadata": {
        "id": "ltIlBt5Wp10M"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7rGQvTarp3W9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}